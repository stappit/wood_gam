---
title: "Chapter 2: Mixed Models"
author: "Brian Callander"
date: "February 27, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA, warning = FALSE, message = FALSE)

library(tidyverse)
library(broom)
```

## Exercise 1

a. The mixed model can be specified as $y_i = \alpha + b_{j[i]} + \epsilon_i$, where $b_{j[i]} \sim \mathcal N (0, \sigma_b^2)$ and $\epsilon_i \sim \mathcal N (0, \sigma^2)$.

b. The `anova` call tests the hypothesis that all $b_{j[i]} = 0$. In the mixed-model setup, this is equivalent to $\sigma_b^2 = 0$.

c. There is strong evidence for between-sow variation, i.e. that $\sigma_b^2 > 0$.

d. An unbiased estimator for between-sow variation is $\hat\sigma_b^2 = \hat\sigma_0^2 - \hat\sigma^2 / J = 0.6351067 - 0.2131667 / 6 = `r 0.6351067 - 0.2131667 / 6`$.

## Exercise 2


We have $a = \alpha + J^{-1} \sum_1^J c_j$ and $e_i = b_i + J^{-1} \sum_1^J \epsilon_{ij}$. The errors $e_i$ are mutually independent since the constituent random variables are. Since $\mathbb V (e_i) = \sigma_b^2 + J^{-1} \sigma^2$, we have 

$$
\newcommand{\rss}{\mathop{RSS}}
\hat \sigma_b^2 = \frac{\rss_0}{I - 1} - \frac{\hat\sigma^2}{J}
$$

where $\rss_0$ is the residual sum of squares for $\bar y_{i.} = a + e_i$.

The above argument is symmetric for $\hat\sigma_c^2$, which can be estimated by

$$
\hat \sigma_c^2 = \frac{\rss_0}{J - 1} - \frac{\hat\sigma^2}{I}
$$

where $\rss_0$ is now the residual sum of squares for $\bar y_{.j} = a' + e_j'$.

## Exercise 3

a. The final row of `anova(lm(y ~ 1 + x + a))` would test the hypothesis.

b. We could fit the following two models

```r
m1 <- lm(y ~ 1 + x + a, data = df)
dfa <- df %>% 
  group_by(a) %>%
  summarise(
    y = mean(y),
    x = mean(x)
  ) 
m0 <- lm(y ~ x, data = dfa)
```

then estimate $\beta$ as the coefficient of $x$ in `m0`, and $\sigma_a^2$ as described in the chapter.

## Exercise 4

a.

$$
\begin{pmatrix}
  y_{11} \\
  y_{12} \\
  y_{21} \\
  y_{22} \\
  y_{31} \\
  y_{32} \\
  y_{41} \\
  y_{42}
\end{pmatrix}
=
\begin{pmatrix}
  1 & x_{11} \\
  1 & x_{12} \\
  1 & x_{21} \\
  1 & x_{22} \\
  1 & x_{31} \\
  1 & x_{32} \\
  1 & x_{41} \\
  1 & x_{42}
\end{pmatrix}
\begin{pmatrix}
  \alpha \\
  \beta
\end{pmatrix}
+
\begin{pmatrix}
  1 & 0 & 0 & 0 \\
  1 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 0 & 1 & 0 \\
  0 & 0 & 1 & 0 \\
  0 & 0 & 0 & 1 \\
  0 & 0 & 0 & 1 
\end{pmatrix}
\begin{pmatrix}
  a_1 \\
  a_2 \\
  a_3 \\
  a_4
\end{pmatrix}
+
\begin{pmatrix}
  \epsilon_{11} \\
  \epsilon_{12} \\
  \epsilon_{21} \\
  \epsilon_{22} \\
  \epsilon_{31} \\
  \epsilon_{32} \\
  \epsilon_{41} \\
  \epsilon_{42}
\end{pmatrix}
$$


b. There are 2 treatment levels $\alpha$, 4 trees $b$ with 2 per treatment level, and 4 measurements per tree.

$$
\begin{pmatrix}
  y_{11} \\
  y_{12} \\
  y_{13} \\
  y_{14} \\
  y_{21} \\
  y_{22} \\
  y_{23} \\
  y_{24} \\
  y_{31} \\
  y_{32} \\
  y_{33} \\
  y_{34} \\
  y_{41} \\
  y_{42} \\
  y_{43} \\
  y_{44}
\end{pmatrix}
=
\begin{pmatrix}
  1 & 0 \\
  1 & 0 \\
  1 & 0 \\
  1 & 0 \\
  1 & 0 \\
  1 & 0 \\
  1 & 0 \\
  1 & 0 \\
  0 & 1 \\
  0 & 1 \\
  0 & 1 \\
  0 & 1 \\
  0 & 1 \\
  0 & 1 \\
  0 & 1 \\
  0 & 1
\end{pmatrix}
\begin{pmatrix}
  \alpha_0 \\
  \alpha_1
\end{pmatrix}
+
\begin{pmatrix}
  1 & 0 & 0 & 0 \\
  1 & 0 & 0 & 0 \\
  1 & 0 & 0 & 0 \\
  1 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 0 & 1 & 0 \\
  0 & 0 & 1 & 0 \\
  0 & 0 & 1 & 0 \\
  0 & 0 & 1 & 0 \\
  0 & 0 & 0 & 1 \\
  0 & 0 & 0 & 1 \\
  0 & 0 & 0 & 1 \\
  0 & 0 & 0 & 1 
\end{pmatrix}
\begin{pmatrix}
  b_0 \\
  b_1 \\
  b_2 \\
  b_3
\end{pmatrix}
+
\begin{pmatrix}
  \epsilon_{11} \\
  \epsilon_{12} \\
  \epsilon_{13} \\
  \epsilon_{14} \\
  \epsilon_{21} \\
  \epsilon_{22} \\
  \epsilon_{23} \\
  \epsilon_{24} \\
  \epsilon_{31} \\
  \epsilon_{32} \\
  \epsilon_{33} \\
  \epsilon_{34} \\
  \epsilon_{41} \\
  \epsilon_{42} \\
  \epsilon_{43} \\
  \epsilon_{44}
\end{pmatrix}
$$



c.

$$
\begin{pmatrix}
  y_{111} \\
  y_{112} \\
  y_{113} \\
  y_{121} \\
  y_{122} \\
  y_{123} \\
  y_{131} \\
  y_{132} \\
  y_{133} \\
  y_{212} \\
  y_{211} \\
  y_{212} \\
  y_{221} \\
  y_{222} \\
  y_{221} \\
  y_{232} \\
  y_{231} \\
  y_{232}
\end{pmatrix}
=
\begin{pmatrix}
  1 & 0 \\
  1 & 0 \\
  1 & 0 \\
  1 & 0 \\
  1 & 0 \\
  1 & 0 \\
  1 & 0 \\
  1 & 0 \\
  1 & 0 \\
  1 & 1 \\
  1 & 1 \\
  1 & 1 \\
  1 & 1 \\
  1 & 1 \\
  1 & 1 \\
  1 & 1 \\
  1 & 1 \\
  1 & 1
\end{pmatrix}
\begin{pmatrix}
  \mu \\
  \alpha_2
\end{pmatrix}
+
\begin{pmatrix}
  1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
  1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
  1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
  0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 \\
  0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 \\
  0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 \\
  1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
  1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
  1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
  0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
  0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
  0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
  0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 \\
  0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 \\
  0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 
\end{pmatrix}
\begin{pmatrix}
  b_1 \\
  b_2 \\
  b_3 \\
  c_{11} \\
  c_{12} \\
  c_{23} \\
  c_{21} \\
  c_{32} \\
  c_{33}
\end{pmatrix}
+
\begin{pmatrix}
  \epsilon_{111} \\
  \epsilon_{112} \\
  \epsilon_{113} \\
  \epsilon_{121} \\
  \epsilon_{122} \\
  \epsilon_{123} \\
  \epsilon_{131} \\
  \epsilon_{132} \\
  \epsilon_{133} \\
  \epsilon_{212} \\
  \epsilon_{211} \\
  \epsilon_{212} \\
  \epsilon_{221} \\
  \epsilon_{222} \\
  \epsilon_{221} \\
  \epsilon_{232} \\
  \epsilon_{231} \\
  \epsilon_{232}
\end{pmatrix}
$$

## Exercise 5

a. With $\mu := \mathbb E (X + Z) = \mu_X + \mu_Z$ and 

$$
\newcommand{\cov}{\mathop{Cov}}
\begin{align}
  \cov (X + Z)
  &=
  \cov X + \cov Z + \cov (X, Z) \cov (X, Z)^T
\end{align}
$$
  
it is sufficient to show that $\cov (X, Z) = 0$. Since $X, Z$ are independent, it follows that $\mathbb E (XZ^T) = \mathbb E X \mathbb E Z^T = \mu_X \mu_Z^T$. Therefore,

$$
\begin{align}
  \cov (X, Z)
  &=
  \mathbb E \left( (X - \mu_X) (Z - \mu_Z)^T \right)
  \\
  &=
  \mathbb E \left( XZ^T + \mu_X \mu_Z^T - X\mu_Z^T - \mu_X Z^T \right)
  \\
  &=
  \mu_X \mu_Z^T + \mu_X \mu_Z^T - \mu_X \mu_Z^T - \mu_X \mu_Z^T 
  \\
  &=
  0
\end{align}
$$

b. The matrix form of the model is

$$
\begin{pmatrix}
  y_{11} \\
  y_{12} \\
  y_{13} \\
  y_{21} \\
  y_{22} \\
  y_{23} \\
  y_{31} \\
  y_{32} \\
  y_{33}
\end{pmatrix}
=
\begin{pmatrix}
  1 & x_{11} \\
  1 & x_{12} \\
  1 & x_{13} \\
  1 & x_{21} \\
  1 & x_{22} \\
  1 & x_{23} \\
  1 & x_{31} \\
  1 & x_{32} \\
  1 & x_{33}
\end{pmatrix}
\begin{pmatrix}
  \alpha \\
  \beta
\end{pmatrix}
+
\begin{pmatrix}
  1 & 0 & 0 \\
  1 & 0 & 0 \\
  1 & 0 & 0 \\
  0 & 1 & 0 \\
  0 & 1 & 0 \\
  0 & 1 & 0 \\
  0 & 0 & 1 \\
  0 & 0 & 1 \\
  0 & 0 & 1
\end{pmatrix}
\begin{pmatrix}
  b_1 \\
  b_2 \\
  b_3
\end{pmatrix}
+
\begin{pmatrix}
  \epsilon_{11} \\
  \epsilon_{12} \\
  \epsilon_{13} \\
  \epsilon_{21} \\
  \epsilon_{22} \\
  \epsilon_{23} \\
  \epsilon_{31} \\
  \epsilon_{32} \\
  \epsilon_{33}
\end{pmatrix}
$$

$$
\begin{align}
  \cov y
  &=
  \cov (Zb + \epsilon)
  \\
  &=
  \cov (Zb) + \cov \epsilon
  \\
  &=
  Z\cov (b) Z^T + \sigma^2
  \\
  &=
  \sigma_b^2 ZZ^T + \sigma^2
\end{align}
$$


$$
ZZ^T
=
\begin{pmatrix}
  1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
  1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
  1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
  0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\
  0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\
  0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\
  0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 \\
  0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 \\
  0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 
\end{pmatrix}
$$

$$
\cov y
=
\begin{pmatrix}
  \sigma_b^2 + \sigma & \sigma_b^2 & \sigma_b^2   & 0 & 0 & 0 & 0 & 0 & 0 \\
  \sigma_b^2 & \sigma_b^2 + \sigma^2 & \sigma_b^2 & 0 & 0 & 0 & 0 & 0 & 0 \\
  \sigma_b^2 & \sigma_b^2 & \sigma_b^2 + \sigma^2 & 0 & 0 & 0 & 0 & 0 & 0 \\
  0 & 0 & 0 & \sigma_b^2 + \sigma & \sigma_b^2 & \sigma_b^2   & 0 & 0 & 0 \\
  0 & 0 & 0 & \sigma_b^2 & \sigma_b^2 + \sigma^2 & \sigma_b^2 & 0 & 0 & 0 \\
  0 & 0 & 0 & \sigma_b^2 & \sigma_b^2 & \sigma_b^2 + \sigma^2 & 0 & 0 & 0 \\
  0 & 0 & 0 & 0 & 0 & 0 & \sigma_b^2 + \sigma & \sigma_b^2 & \sigma_b^2   \\
  0 & 0 & 0 & 0 & 0 & 0 & \sigma_b^2 & \sigma_b^2 + \sigma^2 & \sigma_b^2 \\
  0 & 0 & 0 & 0 & 0 & 0 & \sigma_b^2 & \sigma_b^2 & \sigma_b^2 + \sigma^2 
\end{pmatrix}
$$



